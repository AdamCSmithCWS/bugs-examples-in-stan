Engines: right-censored failure times - the I(,) construct contrasted with other approaches.

The following example comes from Tanner's Tools for Statistical Inference (3rd edition, p67), and demonstrates some of the limitations of WinBUGS.  Forty motors were tested at various operating temperatures, and failure times were noted; 23 of the motors had not failed at the end of the observational period (i.e., the failure times for these observations are right-censored).    Substantive interest is in the regression of failure times on operating temperature; the statistical issue is how to obtain estimates of the regression parameters given the right-censoring.

In WinBUGS, we can use the I(,) construct to deal with the right-censoring, as laid out in the first block of code, with the dependent variables coded as missing (NA) for the censored observations, and the right-censored failure time appearing in the I(,) construct.  Indeed, this use of the I(,) construct would seem to be one of the accepted uses, as laid out in the WinBUGS User Manual (section 3.3), since the actual failure time for these observations is unobserved. Nonetheless, the posterior means reported by WinBUGS (with vague priors) differ slightly from the MLEs (see table, below).

We can also attack this problem via the "ones trick", forming a term equal to the likelihood contribution for the censored observations, and having WinBUGS use this as the parameter for a Bernoulli distribution for a vector of ones.  This trick comes in handy in other truncated data situations where the I(,) construct can't be used.   Yet in this instance, the results are identical to those obtained with the I(,) construct, and differ slightly from the MLEs.  

For comparison, I also present the MLEs and the results of running a Gibbs sampler for this problem in R (quite simple, for this problem), again using vague priors.   Note that this (non-WinBUGS) Gibbs sampler produces results that are extremely close to the MLEs.

				I(,) construct	"ones" trick		MLE		Gibbs (coded in R)
b[1]		-6.20					-6.21				-6.02		-6.07
sd			(1.12)				 (1.12)			   (0.95)    (1.00)

b[2]		4.41					 4.41				  4.31		4.33
sd		  (0.52)				  (0.52)			  (0.44)    (0.46)

Burn-in:  1,000             10,000			  -			  1,000
Thinning: 1						10				     -			  1
Samples: 5,000				9,000				 -			  5,000

Version using the I(,) construct:
model{
  for (i in 1:N.OK){            ## loop over complete observations
    mu[i] <- b[1] + x[i]*b[2]
    y[i] ~ dnorm(mu[i],tau)     ## normal likelihood
  }

  for (i in (N.OK+1):N){        ## loop over censored observations
    mu[i] <- b[1] + x[i]*b[2]
    z[i] ~ dnorm(mu[i],tau)I(cens[i],)   ## left-truncated normal
  }

  ## priors
  b[1:2] ~ dmnorm(b0[], Omega[ , ])       ## multivariate normal
  b0[1] <- 0.0; b0[2] <- 0.0              ## uninformative priors
  Omega[1,1] <- .00001  Omega[1,2] <- 0.0
  Omega[2,1] <- 0.0   Omega[2,2] <- .00001
  tau ~ dgamma(.01,.01)                 ## uninformative Gamma 
  sigma <- sqrt(1/tau)
} 

Data:

list(y=c(3.2464985807958, 3.44279322593977, 3.53706314278162, 3.54924855685406, 3.57749179983722, 3.68663626926229, 3.71566914240099, 2.61066016308988, 2.61066016308988, 3.12839926871781, 3.12839926871781, 3.15836249209525, 2.61066016308988, 2.61066016308988, 2.70243053644553, 2.70243053644553, 2.70243053644553, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA), x=c(2.25631768953069, 2.25631768953069, 2.25631768953069, 2.25631768953069, 2.25631768953069, 2.25631768953069, 2.25631768953069, 2.15889464594128, 2.15889464594128, 2.15889464594128, 2.15889464594128, 2.15889464594128, 2.02757502027575, 2.02757502027575, 2.02757502027575, 2.02757502027575, 2.02757502027575, 2.36294896030246, 2.36294896030246, 2.36294896030246, 2.36294896030246, 2.36294896030246, 2.36294896030246, 2.36294896030246, 2.36294896030246, 2.36294896030246, 2.36294896030246, 2.25631768953069, 2.25631768953069, 2.25631768953069, 2.15889464594128, 2.15889464594128, 2.15889464594128, 2.15889464594128, 2.15889464594128, 2.02757502027575, 2.02757502027575, 2.02757502027575, 2.02757502027575, 2.02757502027575), cens=c(3.2464985807958, 3.44279322593977, 3.53706314278162, 3.54924855685406, 3.57749179983722, 3.68663626926229, 3.71566914240099, 2.61066016308988, 2.61066016308988, 3.12839926871781, 3.12839926871781, 3.15836249209525, 2.61066016308988, 2.61066016308988, 2.70243053644553, 2.70243053644553, 2.70243053644553, 3.90655051910145, 3.90655051910145, 3.90655051910145, 3.90655051910145, 3.90655051910145, 3.90655051910145, 3.90655051910145, 3.90655051910145, 3.90655051910145, 3.90655051910145, 3.73623709890473, 3.73623709890473, 3.73623709890473, 3.22530928172586, 3.22530928172586, 3.22530928172586, 3.22530928172586, 3.22530928172586, 2.72263392253381, 2.72263392253381, 2.72263392253381, 2.72263392253381, 2.72263392253381), N.OK=17, N=40)
 

Initial Values:

list(b=c(-6,4),tau=100)

list(b=c(0,0),tau=1)



Version using the "ones trick" and Metropolis sampling

model{
	  for(i in 1:N){
	    mu[i] <- b[1] +b[2]*(x[i]-mean(x[]))
	  }

  for (i in 1:N.OK){            ## loop over complete observations
    y[i] ~ dnorm(mu[i],tau)     ## normal data and log-likelihood
  }

  for (i in (N.OK+1):N){        ## loop over censored observations
    ones[i] <- 1
    ones[i] ~ dbern(p[i])
	    p[i] <- 1 - phi((cens[i]-mu[i])/sigma)  ## likelihood contribution
	    
  }

  ## priors
  b[1:2] ~ dmnorm(b0[], Omega[ , ])       ## multivariate normal
  b0[1] <- 0.0; b0[2] <- 0.0              ## uninformative priors
  Omega[1,1] <- .00001  Omega[1,2] <- 0.0
  Omega[2,1] <- 0.0   Omega[2,2] <- .00001
  tau ~ dgamma(.01,.01)                   ## uninformative Gamma 
  sigma <- sqrt(1/tau)

  	alpha <- b[1] - b[2]*mean(x[])
} 



Results:  Iterations 10,001 to 100,000, with thinning by 10.
	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	alpha	-6.211	1.118	0.01183	-8.595	-6.167	-4.17	10001	9000
	b[1]	3.5	0.07134	7.823E-4	3.381	3.493	3.66	10001	9000
	b[2]	4.411	0.5171	0.005527	3.492	4.387	5.525	10001	9000
	sigma	0.2978	0.06327	8.091E-4	0.204	0.2881	0.4484	10001	9000




