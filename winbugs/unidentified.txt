Unidentified: over-parameterization of normal mean

The following example illustrates the need for caution in diagnosing convergence, and is based on an example appearing in Carlin and Louis' Bayes and Empirical Bayes Methods for Data Analysis, 2nd edition, p174.

Consider a model for the mean as an additive sum of two parameters: e.g., 
y ~ N(q1+ q2, 1).   The data are not informative about q1 and q2 , but are informative about m = q1 + q2  and the likelihood function for the two unidentified parameters has a ridge along the locus of points 

(q1,q2): ybar = q1 + q2

where ybar is the mean of the observed data.  

In the Bayesian approach, we are obliged to specify priors over the model parameters.  Proper priors ensure unimodal posteriors for q1 and q2, and normal priors ensure conjugacy and a simple Gibbs sampler (with normal conditionals) can be used to the sample from the posterior for this problem.  But as Carlin and Louis show (see their Q25, p191), we need to be careful with models of this type.  The posteriors for theta are not identical to the prior (the posterior standard deviations are 7.05, while the prior standard deviations used below are 10), suggesting that the data are somewhat informative about both theta parameters, when this is not the case.  An inexperienced user of Markov chain Monte Carlo methods might fail to recognize that the q parameters are not identified, and naively report the posterior summaries for theta generated by the software.  On the other hand, note that the identified parameter m = q1 + q2  is well behaved.

The problems with this model become more exacerbated as the priors tend towards impropriety; see the results from using N(0,10000) priors, but en bloc updating of the theta parameters appears to mitigate the slow mixing that results from treating each component of theta as a distinct node.

model{

	## loop over observations
	for (i in 1:1){
		y[i] ~ dnorm(mu,1.0);  ## known precision
	}		
	mu <- theta[1] + theta[2]
	
	## priors
	#theta[1] ~ dnorm(0.0, 0.01);     ## this form is not efficient
	#theta[2] ~ dnorm(0.0, 0.01);     ## vis-a-vis en bloc approach below
	
	theta[1:2] ~ dmnorm(b0[],B0[,])   ## en bloc updating for theta
	b0[1] <- 0 b0[2] <- 0
	B0[1,1] <- .01 B0[2,2] <- .01
	B0[1,2] <- 0 B0[2,1] <- 0
}

Data:
list(y=c(0))
Initial Values:
list(theta=c(0,0))

Results with univariate N(0,100) priors:


	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	mu	-0.001947	0.9758	0.001686	-1.907	-0.002111	1.915	10001	180000
	theta[1]	-0.007469	2.293	0.01711	-4.508	-0.00116	4.46	10001	180000
	theta[2]	0.005523	2.291	0.01728	-4.461	0.002011	4.5	10001	180000

Densities:


Scatterplot for theta:



Autocorrelations:




Results with univariate N(0,10000) priors:

	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	mu	-0.001893	0.9972	0.001825	-1.954	-0.002808	1.955	10001	180000
	theta[1]	-0.05764	7.038	0.1513	-14.08	-0.02319	13.72	10001	180000
	theta[2]	0.05574	7.037	0.1515	-13.73	0.01983	14.1	10001	180000





Results with multivariate N(0,10000) priors:

	 node	 mean	 sd	 MC error	2.5%	median	97.5%	start	sample
	mu	-0.002634	0.995	0.002255	-1.953	-0.002252	1.949	10001	180000
	theta[1]	0.00679	7.085	0.01736	-13.88	0.01368	13.91	10001	180000
	theta[2]	-0.009424	7.086	0.01747	-13.87	-0.0136	13.91	10001	180000

	 
Autocorrelations:


