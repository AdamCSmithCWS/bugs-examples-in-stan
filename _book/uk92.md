
# House of Commons elections: modeling with the multivariate t density {#uk92}



```r
library("tidyverse")
library("rstan")
```


The following analysis comes from a data set on United Kingdom House of Commons elections, used in @KatzKing1999a, and re-analyzed in @TomzTuckerWittenberg2002a.  The data for this example consist of vote proportions from 1992 for 521 constituencies, for the Labor party, the Conservative Party, and the Liberal-Alliance, transformed to a 521 by 2 matrix of log-odds ratios (with the Liberal-Alliance in the denominator), as is common in the analysis of multinomial or "compositional" data [@Aitchison1982a].  @KatzKing1999a noted that the distribution of the log-odds ratios appear to be heavy-tailed relative to the normal, and model the data with a multivariate t-distribution with unknown degrees of freedom.

This specification includes covariates for the conditional means of the two log odds ratios.  Six covariates are used: (1) a constant; (2) and (3) the lagged values (from the previous election) of the two log odds ratios; (4) an indicator for whether the Conservative candidate is the incumbent; (5) the incumbency status of the Labor candidate; (6) the incumbency status of the Liberal/Alliance candidate.  

$$
\begin{aligned}[t]
y_i &\sim \mathsf{StudentT}(\nu, \alpha + x' \beta, \Sigma) .
\end{aligned}
$$
where $\Sigma$ is a correlation matrix.
$y_i$ are log-odds.

For identifiction, as in a logit regression, either the intercept or scale must be fixed.
In this case, $\Sigma$ is a correlation matrix.

Weakly informative priors are used for the regression parameters. 
The degrees of freedom of the multivariate Student t distribution is a parameter, and given a weakly informative Gamma distribution that puts most of the prior density between 3 and 40 [@JuarezSteel2010a].
$$
\begin{aligned}[t]
\alpha &\sim  \mathsf{Normal}(0, 10) \\
\beta_p &\sim \mathsf{Normal}(0, 2.5) & \text{for $p \in 1, \dots, P$.} \\
\Sigma &\sim \mathsf{LkjCorr}(\eta) \\
\nu &\sim \mathsf{Gamma}(2, 0.1) 
\end{aligned}
$$


```r
uk92_data <- within(dget("data/uk92.R"), {
  X <- scale(x)
  x <- NULL
  N <- nrow(y)
  K <- ncol(y)
  P <- ncol(X)
  a_location <- rep(0, K)
  a_scale <- rep(10, K)
  b_location <- matrix(0, K, P)
  b_scale <- matrix(2.5, K, P)
  Sigma_shape <- 1
})
```


```r
uk92_mod <- stan_model("stan/uk92.stan")
```
<pre>
  <code class="stan">data {
  // multivariate outcome
  int<lower = 1> N;
  int<lower = 2> K;
  vector[K] y[N];
  // covariates
  int<lower = 0> P;
  vector[P] X[N];
  // prior
  vector[K] a_location;
  vector<lower = 0.>[K] a_scale;
  vector[P] b_location[K];
  vector<lower = 0.>[P] b_scale[K];
  real<lower = 0.> Sigma_shape;
}
parameters {
  vector[K] a;
  vector[P] b[K];
  corr_matrix[K] Sigma;
  real<lower = 2.> nu;
}
transformed parameters {
  vector[K] mu[N];
  for (i in 1:N) {
    for (k in 1:K) {
      mu[i, k] = a[k] + dot_product(X[i], b[k]);
    }
  }
}
model {
  for (k in 1:K) {
    a[k] ~ normal(a_location[k], a_scale[k]);
    b[k] ~ normal(b_location[k], b_scale[k]);
  }
  nu ~ gamma(2, 0.1);
  Sigma ~ lkj_corr(Sigma_shape);
  for (i in 1:N) {
    y[i] ~ multi_student_t(nu, mu[i], Sigma);
  }
}</code>
</pre>



```r
uk92_fit <- sampling(uk92_mod, data = uk92_data)
```

```r
summary(uk92_fit, par = c("nu", "a", "b", "Sigma"))$summary
#>                 mean  se_mean       sd    2.5%      25%       50%      75%
#> nu         58.799006 3.01e-01 1.90e+01 30.1925 45.06379 56.213941 69.89224
#> a[1]        0.914937 6.98e-04 4.41e-02  0.8304  0.88479  0.915408  0.94453
#> a[2]        0.637108 7.01e-04 4.43e-02  0.5508  0.60700  0.637370  0.66721
#> b[1,1]     -0.049963 7.27e-04 4.60e-02 -0.1395 -0.08010 -0.050882 -0.01873
#> b[1,2]     -0.009028 7.16e-04 4.53e-02 -0.0977 -0.03885 -0.009309  0.02050
#> b[1,3]     -0.000209 7.36e-04 4.65e-02 -0.0894 -0.03234  0.000646  0.03126
#> b[1,4]      0.005203 7.99e-04 5.06e-02 -0.0957 -0.02875  0.005764  0.03970
#> b[1,5]     -0.005391 7.72e-04 4.88e-02 -0.0999 -0.03905 -0.004848  0.02658
#> b[1,6]     -0.072256 7.75e-04 4.90e-02 -0.1674 -0.10513 -0.072789 -0.03973
#> b[2,1]     -0.121483 7.28e-04 4.60e-02 -0.2103 -0.15211 -0.121901 -0.09043
#> b[2,2]     -0.050414 7.33e-04 4.63e-02 -0.1420 -0.08221 -0.050696 -0.01938
#> b[2,3]     -0.079795 7.31e-04 4.62e-02 -0.1677 -0.11069 -0.080478 -0.04888
#> b[2,4]     -0.037566 8.02e-04 5.07e-02 -0.1351 -0.07275 -0.037616 -0.00278
#> b[2,5]      0.105411 7.63e-04 4.83e-02  0.0121  0.07186  0.105012  0.13886
#> b[2,6]      0.023125 7.61e-04 4.81e-02 -0.0706 -0.00905  0.023781  0.05486
#> Sigma[1,1]  1.000000 0.00e+00 0.00e+00  1.0000  1.00000  1.000000  1.00000
#> Sigma[1,2]  0.021121 1.23e-03 7.79e-02 -0.1318 -0.03164  0.019927  0.07558
#> Sigma[2,1]  0.021121 1.23e-03 7.79e-02 -0.1318 -0.03164  0.019927  0.07558
#> Sigma[2,2]  1.000000 1.54e-18 9.71e-17  1.0000  1.00000  1.000000  1.00000
#>               97.5% n_eff  Rhat
#> nu         104.5609  4000 1.000
#> a[1]         1.0024  4000 1.000
#> a[2]         0.7243  4000 0.999
#> b[1,1]       0.0402  4000 0.999
#> b[1,2]       0.0821  4000 1.000
#> b[1,3]       0.0874  4000 0.999
#> b[1,4]       0.1026  4000 1.000
#> b[1,5]       0.0907  4000 1.000
#> b[1,6]       0.0236  4000 0.999
#> b[2,1]      -0.0307  4000 0.999
#> b[2,2]       0.0401  4000 0.999
#> b[2,3]       0.0135  4000 1.000
#> b[2,4]       0.0600  4000 1.001
#> b[2,5]       0.2006  4000 1.000
#> b[2,6]       0.1181  4000 1.000
#> Sigma[1,1]   1.0000  4000   NaN
#> Sigma[1,2]   0.1761  4000 1.000
#> Sigma[2,1]   0.1761  4000 1.000
#> Sigma[2,2]   1.0000  4000 0.999
```


This differs slightly from the original Jackman model.
The Jackman model omitted an intercept, but did not fix the scale of the Sigma distribution.
It also used several prior distributions better suited for WinBUGS.
